{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertModel, AutoTokenizer, RobertaModel, RobertaTokenizerFast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, normalize\n",
    "import torch, os, random, copy\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from ogb.graph_aug import mask_nodes, mask_edges, permute_edges, drop_nodes, subgraph\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot as plt\n",
    "from torch_ema import ExponentialMovingAverage\n",
    "from ogb.utils import smiles2graph\n",
    "from models.dualgraph.mol import smiles2graphwithface, simles2graphwithface_with_mask\n",
    "from models.dualgraph.gnn import one_hot_atoms, one_hot_bonds, GNN2\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import Chem\n",
    "from torch_geometric.data import Dataset, InMemoryDataset\n",
    "from models.dualgraph.dataset import DGData\n",
    "\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ring(data, node_mask, edge_mask):\n",
    "    ring_index = data.ring_index[:, edge_mask]\n",
    "    data.ring_index = ring_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2705491510.py, line 55)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[76], line 55\u001b[0;36m\u001b[0m\n\u001b[0;31m    return_edge_mask=True)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from itertools import repeat\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import subgraph, to_networkx\n",
    "\n",
    "class MoleculeDataset_graphcl(InMemoryDataset):\n",
    "    def __init__(self,\n",
    "                 root='dataset_path',\n",
    "                 transform=None,\n",
    "                 pre_transform=None, \n",
    "                 df=None):\n",
    "        self.df = df\n",
    "        self.aug_prob = None\n",
    "        self.aug_mode = 'sample'\n",
    "        self.aug_strength = 0.2\n",
    "        self.augmentations = [self.node_drop, self.subgraph,\n",
    "                              self.edge_pert, self.attr_mask, lambda x: x]\n",
    "        super().__init__(root, transform, pre_transform, df)\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [f'raw_{i+1}.pt' for i in range(self.df.shape[0])]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [f'data_{i+1}.pt' for i in range(self.df.shape[0])]\n",
    "    \n",
    "    def set_augMode(self, aug_mode):\n",
    "        self.aug_mode = aug_mode\n",
    "\n",
    "    def set_augStrength(self, aug_strength):\n",
    "        self.aug_strength = aug_strength\n",
    "\n",
    "    def set_augProb(self, aug_prob):\n",
    "        self.aug_prob = aug_prob\n",
    "\n",
    "    def node_drop(self, data):\n",
    "\n",
    "        node_num, _ = data.x.size()\n",
    "        _, edge_num = data.edge_index.size()\n",
    "        drop_num = int(node_num * self.aug_strength)\n",
    "\n",
    "        idx_perm = np.random.permutation(node_num)\n",
    "        idx_nodrop = idx_perm[drop_num:].tolist()\n",
    "        idx_nodrop.sort()\n",
    "\n",
    "        edge_idx, edge_attr, edge_mask = subgraph(subset=idx_nodrop,\n",
    "                                       edge_index=data.edge_index,\n",
    "                                       edge_attr=data.edge_attr,\n",
    "                                       relabel_nodes=True,\n",
    "                                       num_nodes=node_num,\n",
    "                                       return_edge_mask=True)                        \n",
    "\n",
    "        data.edge_index = edge_idx\n",
    "        data.edge_attr = edge_attr\n",
    "        data.x = data.x[idx_nodrop]\n",
    "        data.__num_nodes__, _ = data.x.shape\n",
    "        return data\n",
    "\n",
    "    def edge_pert(self, data):        \n",
    "        node_num, _ = data.x.size()\n",
    "        _, edge_num = data.edge_index.size()\n",
    "        pert_num = int(edge_num * self.aug_strength)\n",
    "\n",
    "        # delete edges\n",
    "        idx_drop = np.random.choice(edge_num, (edge_num - pert_num),\n",
    "                                    replace=False)\n",
    "        edge_index = data.edge_index[:, idx_drop]\n",
    "        edge_attr = data.edge_attr[idx_drop]\n",
    "\n",
    "        # add edges\n",
    "        adj = torch.ones((node_num, node_num))\n",
    "        adj[edge_index[0], edge_index[1]] = 0\n",
    "        # edge_index_nonexist = adj.nonzero(as_tuple=False).t()\n",
    "        edge_index_nonexist = torch.nonzero(adj, as_tuple=False).t()\n",
    "        idx_add = np.random.choice(edge_index_nonexist.shape[1],\n",
    "                                    pert_num, replace=False)\n",
    "        edge_index_add = edge_index_nonexist[:, idx_add]\n",
    "        # random 4-class & 3-class edge_attr for 1st & 2nd dimension\n",
    "        edge_attr_add_1 = torch.tensor(np.random.randint(\n",
    "            4, size=(edge_index_add.shape[1], 1)))\n",
    "        edge_attr_add_2 = torch.tensor(np.random.randint(\n",
    "            3, size=(edge_index_add.shape[1], 1)))\n",
    "        edge_attr_add_3 = torch.tensor(np.random.randint(\n",
    "            2, size=(edge_index_add.shape[1], 1)))\n",
    "        edge_attr_add = torch.cat((edge_attr_add_1, edge_attr_add_2, edge_attr_add_3), dim=1)\n",
    "        edge_index = torch.cat((edge_index, edge_index_add), dim=1)\n",
    "        \n",
    "\n",
    "        edge_attr = torch.cat((edge_attr, edge_attr_add), dim=0)\n",
    "\n",
    "        data.edge_index = edge_index\n",
    "        data.edge_attr = edge_attr\n",
    "        return data\n",
    "\n",
    "    def attr_mask(self, data):\n",
    "\n",
    "        _x = data.x.clone()\n",
    "        node_num, _ = data.x.size()\n",
    "        mask_num = int(node_num * self.aug_strength)\n",
    "\n",
    "        token = data.x.float().mean(dim=0).long()\n",
    "        idx_mask = np.random.choice(\n",
    "            node_num, mask_num, replace=False)\n",
    "\n",
    "        _x[idx_mask] = token\n",
    "        data.x = _x\n",
    "        return data\n",
    "\n",
    "    def subgraph(self, data):\n",
    "\n",
    "        G = to_networkx(data)\n",
    "        node_num, _ = data.x.size()\n",
    "        _, edge_num = data.edge_index.size()\n",
    "        sub_num = int(node_num * (1 - self.aug_strength))\n",
    "\n",
    "        idx_sub = [np.random.randint(node_num, size=1)[0]]\n",
    "        idx_neigh = set([n for n in G.neighbors(idx_sub[-1])])\n",
    "\n",
    "        while len(idx_sub) <= sub_num:\n",
    "            if len(idx_neigh) == 0:\n",
    "                idx_unsub = list(set([n for n in range(node_num)]).difference(set(idx_sub)))\n",
    "                idx_neigh = set([np.random.choice(idx_unsub)])\n",
    "            sample_node = np.random.choice(list(idx_neigh))\n",
    "\n",
    "            idx_sub.append(sample_node)\n",
    "            idx_neigh = idx_neigh.union(\n",
    "                set([n for n in G.neighbors(idx_sub[-1])])).difference(set(idx_sub))\n",
    "\n",
    "        idx_nondrop = idx_sub\n",
    "        idx_nondrop.sort()\n",
    "\n",
    "        edge_idx, edge_attr = subgraph(subset=idx_nondrop,\n",
    "                                       edge_index=data.edge_index,\n",
    "                                       edge_attr=data.edge_attr,\n",
    "                                       relabel_nodes=True,\n",
    "                                       num_nodes=node_num)\n",
    "        \n",
    "        data.edge_index = edge_idx\n",
    "        data.edge_attr = edge_attr\n",
    "        data.x = data.x[idx_nondrop]\n",
    "        data.__num_nodes__, _ = data.x.shape\n",
    "        return data\n",
    "\n",
    "    def get(self, idx):\n",
    "        sid = self.sid_list[idx]\n",
    "        dset = self.dset_list[idx]\n",
    "        data = torch.load(f'/home/pjh/workspace/SOM/data/pretrain_graph/{dset}/{sid}.pt')\n",
    "        data1 = copy.deepcopy(data)\n",
    "        data2 = copy.deepcopy(data)\n",
    "\n",
    "        if self.aug_mode == 'no_aug':\n",
    "            n_aug1, n_aug2 = 4, 4\n",
    "            data1 = self.augmentations[n_aug1](data1)\n",
    "            data2 = self.augmentations[n_aug2](data2)\n",
    "        elif self.aug_mode == 'uniform':\n",
    "            n_aug = np.random.choice(25, 1)[0]\n",
    "            n_aug1, n_aug2 = n_aug // 5, n_aug % 5\n",
    "            data1 = self.augmentations[n_aug1](data1)\n",
    "            data2 = self.augmentations[n_aug2](data2)\n",
    "        elif self.aug_mode == 'sample':\n",
    "            n_aug = np.random.choice(25, 1, p=self.aug_prob)[0]\n",
    "            n_aug1, n_aug2 = n_aug // 5, n_aug % 5\n",
    "            data1 = self.augmentations[n_aug1](data1)\n",
    "            data2 = self.augmentations[n_aug2](data2)\n",
    "        else:\n",
    "            raise ValueError\n",
    "        \n",
    "        return data, data1, data2\n",
    "    \n",
    "    def process(self):\n",
    "        self.sid_list = []\n",
    "        self.dset_list = []\n",
    "        \n",
    "        for i in range(self.df.shape[0]):\n",
    "            smile = self.df.loc[i, 'smiles']\n",
    "            dset_name = self.df.loc[i, 'dataset']\n",
    "            sid = self.df.loc[i, 'sid']\n",
    "            self.dset_list.append(dset_name)\n",
    "            self.sid_list.append(sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mol2graph(mol):\n",
    "#     data = DGData()\n",
    "#     graph = smiles2graphwithface(mol)\n",
    "\n",
    "#     data.__num_nodes__ = int(graph[\"num_nodes\"])\n",
    "#     data.edge_index = torch.from_numpy(graph[\"edge_index\"]).to(torch.int64)\n",
    "#     data.edge_attr = torch.from_numpy(graph[\"edge_feat\"]).to(torch.int64)\n",
    "#     data.x = torch.from_numpy(graph[\"node_feat\"]).to(torch.int64)    \n",
    "\n",
    "#     data.ring_mask = torch.from_numpy(graph[\"ring_mask\"]).to(torch.bool)\n",
    "#     data.ring_index = torch.from_numpy(graph[\"ring_index\"]).to(torch.int64)\n",
    "#     data.nf_node = torch.from_numpy(graph[\"nf_node\"]).to(torch.int64)\n",
    "#     data.nf_ring = torch.from_numpy(graph[\"nf_ring\"]).to(torch.int64)\n",
    "#     data.num_rings = int(graph[\"num_rings\"])\n",
    "#     data.n_edges = int(graph[\"n_edges\"])\n",
    "#     data.n_nodes = int(graph[\"n_nodes\"])\n",
    "#     data.n_nfs = int(graph[\"n_nfs\"])\n",
    "\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = False  # type: ignore\n",
    "seed_everything(2023)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.27it/s]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/home/pjh/workspace/MED/sub_code/OGNN/pretrain/zinc_combined_apr_8_2019.csv', index_col=None)\n",
    "data = data[['zinc_id', 'smiles']]\n",
    "data.columns = ['sid', 'smiles']\n",
    "data['dataset'] = 'zinc_combined_apr_8_2019'\n",
    "for dset_name in tqdm(os.listdir( '/home/pjh/workspace/MED/sub_code/OGNN/pretrain')):\n",
    "    if 'zinc_combined_apr_8_2019' in dset_name:\n",
    "        continue\n",
    "    df = pd.read_csv(f'/home/pjh/workspace/MED/sub_code/OGNN/pretrain/{dset_name}', index_col=None)            \n",
    "    \n",
    "    df['dataset'] = dset_name.replace('.csv', '')\n",
    "    df['sid'] = dset_name.replace('.csv', '') + '_' +  df.reset_index()['index'].astype(str)\n",
    "    if 'bace' in dset_name:\n",
    "        df = df[['sid', 'dataset', 'mol']]\n",
    "        df.columns = ['sid', 'dataset', 'smiles']\n",
    "    else:\n",
    "        df = df[['sid', 'dataset', 'smiles']]\n",
    "    data = pd.concat([data, df]).reset_index(drop=True)\n",
    "\n",
    "# data = data.sample(20000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_node_edge(batch_org, p, cyp_list):    \n",
    "    batch = deepcopy(batch_org)\n",
    "    prob = torch.rand(batch.num_nodes, device=batch.edge_index.device)\n",
    "    node_mask = prob > p    \n",
    "    node_mask[batch.y_atom['CYP_REACTION'].bool()] = True\n",
    "\n",
    "    edge_index, _, edge_mask = subgraph(node_mask, batch.edge_index,\n",
    "                                        num_nodes=batch.num_nodes,\n",
    "                                        return_edge_mask=True)\n",
    "    \n",
    "    node_mask_idx = torch.zeros(node_mask.shape[0]).long()\n",
    "    node_mask_idx[node_mask] = torch.arange(node_mask.sum().item())\n",
    "\n",
    "    edge_mask_idx = torch.zeros(edge_mask.shape[0]).long()\n",
    "    edge_mask_idx[edge_mask] = torch.arange(edge_mask.sum().item())\n",
    "    \n",
    "    batch.edge_index = node_mask_idx[edge_index]\n",
    "\n",
    "    ring_index = batch.ring_index[:, edge_mask]\n",
    "    # ring_index = edge_mask_idx[ring_index]\n",
    "\n",
    "    batch.ring_index = ring_index\n",
    "    batch.edge_attr = batch.edge_attr[edge_mask]\n",
    "    \n",
    "    batch.n_edges = edge_mask.sum().item()\n",
    "    batch.n_nodes = node_mask.sum().item()\n",
    "\n",
    "    nf_node_mask = ~torch.isin(batch.nf_node, torch.where(~node_mask)[0])[0]\n",
    "\n",
    "    for ridx in range(batch.num_rings):        \n",
    "        ring_node_mask = nf_node_mask[batch.nf_ring[0] == ridx+1]\n",
    "        if not ring_node_mask.all():\n",
    "            batch.ring_mask[ridx] = False\n",
    "\n",
    "    batch.nf_node = batch.nf_node[:, nf_node_mask]\n",
    "    batch.nf_node = node_mask_idx[batch.nf_node]\n",
    "    batch.nf_ring = batch.nf_ring[:, nf_node_mask]\n",
    "\n",
    "    batch.n_nfs = batch.nf_node.size(1)\n",
    "    \n",
    "    edge_mask = edge_mask.view(edge_mask.shape[0] // 2, 2)\n",
    "    edge_mask = edge_mask[:, 0]\n",
    "\n",
    "    batch.x = batch.x[node_mask]\n",
    "    batch.spn_atom = batch.spn_atom[node_mask]\n",
    "    batch.has_H_atom = batch.has_H_atom[node_mask]\n",
    "    batch.not_has_H_bond = batch.not_has_H_bond[edge_mask]\n",
    "        \n",
    "    for cyp in cyp_list:\n",
    "        batch.y_spn[cyp] = batch.y_spn[cyp][node_mask]\n",
    "        batch.y_atom[cyp] = batch.y_atom[cyp][node_mask]\n",
    "        batch.y_hydroxylation[cyp] = batch.y_hydroxylation[cyp][node_mask]\n",
    "        batch.y_nh_oxidation[cyp] = batch.y_nh_oxidation[cyp][node_mask]\n",
    "        \n",
    "        batch.y[cyp] = batch.y[cyp][edge_mask]\n",
    "        batch.y_cleavage[cyp] = batch.y_cleavage[cyp][edge_mask]\n",
    "        batch.y_nn_oxidation[cyp] = batch.y_nn_oxidation[cyp][edge_mask]\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MoleculeDataset_graphcl(df = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_augMode('sample')\n",
    "train_dataset.set_augProb(np.ones(25) / 25)\n",
    "train_dataset.set_augStrength(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch, batch1, batch2 in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False, False, False, False], device='cuda:0')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch1.ring_mask\n",
    "batch1.nf_node\n",
    "batch1.nf_ring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 46 but got size 66 for tensor number 3 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/crash/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[66], line 37\u001b[0m, in \u001b[0;36mGraphContrastiveLearning.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m---> 37\u001b[0m     mol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(mol)\n",
      "File \u001b[0;32m~/anaconda3/envs/crash/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/SOM/graph_mamba/models/dualgraph/gnn.py:500\u001b[0m, in \u001b[0;36mGNN2.forward\u001b[0;34m(self, batch, perturb, perturb_edge)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_drop \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m droplayer_probs[i] \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_drop:\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 500\u001b[0m x_1, edge_attr_1, u_1, face_1 \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43medge_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[43mface_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m    \u001b[49m\u001b[43mface_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mface_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_faces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_edges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnf_node\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnf_face\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual:\n\u001b[1;32m    519\u001b[0m     x \u001b[38;5;241m=\u001b[39m x_1\n",
      "File \u001b[0;32m~/anaconda3/envs/crash/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/SOM/graph_mamba/models/dualgraph/conv.py:201\u001b[0m, in \u001b[0;36mMetaLayer2.forward\u001b[0;34m(self, x, edge_index, edge_attr, u, node_batch, edge_batch, face_batch, face, face_mask, face_index, num_nodes, num_faces, num_edges, nf_node, nf_face, batch)\u001b[0m\n\u001b[1;32m    199\u001b[0m     feat_list\u001b[38;5;241m.\u001b[39mextend([inface, outface])\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# for i in feat_list:print(i.shape)\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m concat_feat \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual:\n\u001b[1;32m    203\u001b[0m     edge_attr \u001b[38;5;241m=\u001b[39m edge_attr \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_model(concat_feat)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 46 but got size 66 for tensor number 3 in the list."
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(batch1.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mout\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([46, 3])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch1.edge_attr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for dset_name in data['dataset'].unique():\n",
    "# #     os.mkdir(f'/home/pjh/workspace/SOM/data/pretrain_graph/{dset_name}')\n",
    "# for smile, dset_name, sid in tqdm(data[['smiles', 'dataset', 'sid']].values):\n",
    "#     mol = Chem.MolFromSmiles(smile)\n",
    "#     # mol = AllChem.AddHs( mol, addCoords=True)\n",
    "#     graph = mol2graph(mol)\n",
    "#     torch.save(graph,f'/home/pjh/workspace/SOM/data/pretrain_graph/{dset_name}/{sid}.pt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_loss = nn.TripletMarginLoss(margin=0.0, p=2)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphContrastiveLearning(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GraphContrastiveLearning, self).__init__()\n",
    "        self.ddi = True\n",
    "        self.gnn = GNN2(\n",
    "                        mlp_hidden_size = 512,\n",
    "                        mlp_layers = 2,\n",
    "                        num_message_passing_steps=8,\n",
    "                        latent_size = 128,\n",
    "                        use_layer_norm = True,\n",
    "                        use_bn=False,\n",
    "                        use_face=True,\n",
    "                        som_mode=False,\n",
    "                        ddi=True,\n",
    "                        dropedge_rate = 0.1,\n",
    "                        dropnode_rate = 0.1,\n",
    "                        dropout = 0.1,\n",
    "                        dropnet = 0.1,\n",
    "                        global_reducer = 'sum',\n",
    "                        node_reducer = 'sum',\n",
    "                        face_reducer = 'sum',\n",
    "                        graph_pooling = 'sum',\n",
    "                        use_mamba=False,\n",
    "                        node_attn = True,\n",
    "                        face_attn = True,\n",
    "                        encoder_dropout=0.0,\n",
    "                        use_pe=False\n",
    "                        )\n",
    "                        \n",
    "        self.proj = nn.Sequential(\n",
    "                        nn.Linear(128, 128),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(128, 128),\n",
    "                        )\n",
    "    def forward(self, batch):\n",
    "        mol = self.gnn(batch).squeeze(1)\n",
    "        return self.proj(mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "epochs = 100\n",
    "lr = 1e-5\n",
    "weight_decay = 5e-5\n",
    "batch_size= 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MoleculeDataset_graphcl(df = data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers = 8)\n",
    "\n",
    "model = GraphContrastiveLearning().to(device)\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "ema = ExponentialMovingAverage(model.parameters(), decay=0.999)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=epochs, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_cl(x1, x2):\n",
    "    T = 0.1\n",
    "    batch_size, _ = x1.size()\n",
    "    x1_abs = x1.norm(dim=1)\n",
    "    x2_abs = x2.norm(dim=1)\n",
    "\n",
    "    sim_matrix = torch.einsum('ik,jk->ij', x1, x2) / torch.einsum('i,j->ij', x1_abs, x2_abs)\n",
    "    sim_matrix = torch.exp(sim_matrix / T)\n",
    "    pos_sim = sim_matrix[range(batch_size), range(batch_size)]\n",
    "    loss = pos_sim / (sim_matrix.sum(dim=1) - pos_sim)\n",
    "    loss = - torch.log(loss).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(f'/home/pjh/workspace/SOM/graph_mamba/ckpt_pretrain/gnn_pretrain_epoch36.pt')\n",
    "\n",
    "model.load_state_dict(state_dict['model_state_dict'])\n",
    "scheduler.load_state_dict(state_dict['scheduler_state_dict'])\n",
    "ema.load_state_dict(state_dict['ema_state_dict'] )\n",
    "optim.load_state_dict(state_dict['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10113/10113 [1:57:26<00:00,  1.44it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 30 | train_loss : -3.9199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10113/10113 [1:56:08<00:00,  1.45it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 31 | train_loss : -3.9221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10113/10113 [1:52:47<00:00,  1.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 32 | train_loss : -3.9248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10113/10113 [1:52:54<00:00,  1.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 33 | train_loss : -3.9270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10113/10113 [1:52:57<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 34 | train_loss : -3.9289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10113/10113 [1:52:45<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 35 | train_loss : -3.9310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10113/10113 [1:52:51<00:00,  1.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 36 | train_loss : -3.9328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10113/10113 [1:52:58<00:00,  1.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH : 37 | train_loss : -3.9343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 39/10113 [00:29<1:59:15,  1.41it/s]"
     ]
    }
   ],
   "source": [
    "best_val_loss = 1e6\n",
    "start = 1\n",
    "for epoch in range(37, epochs+1):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch = [bat.to(device) for bat in batch]\n",
    "        outputs = [model(bat) for  bat in batch]        \n",
    "        origin_output = outputs[0]\n",
    "        \n",
    "        mask_cl_loss = loss_cl(outputs[1], outputs[2])                    \n",
    "        mask_t_loss = triplet_loss(outputs[0], outputs[1], outputs[2])\n",
    "\n",
    "        loss = mask_cl_loss + (mask_t_loss * 0.1)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward() \n",
    "        optim.step()\n",
    "        ema.update()\n",
    "        \n",
    "        train_loss += loss.cpu().item()\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if train_loss < best_val_loss:\n",
    "        best_val_loss = train_loss\n",
    "        torch.save(model.gnn.state_dict(), f'ckpt_pretrain/gnn_pretrain.pt')\n",
    "        \n",
    "\n",
    "    scheduler.step()\n",
    "    torch.save(\n",
    "            {\n",
    "            'optimizer_state_dict': optim.state_dict(),\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'gnn_state_dict' : model.gnn.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'ema_state_dict' : ema.state_dict()\n",
    "            },\n",
    "            f'ckpt_pretrain/gnn_pretrain_epoch{epoch}.pt')\n",
    "\n",
    "    print(f'EPOCH : {epoch} | train_loss : {train_loss/len(train_loader):.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCH : 1 | train_loss : -2.3468\n",
    "# EPOCH : 2 | train_loss : -3.3075\n",
    "# EPOCH : 3 | train_loss : -3.5078\n",
    "# EPOCH : 4 | train_loss : -3.6116\n",
    "# EPOCH : 5 | train_loss : -3.6767\n",
    "# EPOCH : 6 | train_loss : -3.7220\n",
    "# EPOCH : 7 | train_loss : -3.7547\n",
    "# EPOCH : 8 | train_loss : -3.7785\n",
    "# EPOCH : 9 | train_loss : -3.7976\n",
    "# EPOCH : 10 | train_loss : -3.8130\n",
    "# EPOCH : 11 | train_loss : -3.8248\n",
    "# EPOCH : 12 | train_loss : -3.8358\n",
    "# EPOCH : 13 | train_loss : -3.8443\n",
    "# EPOCH : 14 | train_loss : -3.8521\n",
    "# EPOCH : 15 | train_loss : -3.8593\n",
    "# EPOCH : 16 | train_loss : -3.8657\n",
    "# EPOCH : 17 | train_loss : -3.8720\n",
    "# EPOCH : 18 | train_loss : -3.8774\n",
    "# EPOCH : 19 | train_loss : -3.8831\n",
    "# EPOCH : 20 | train_loss : -3.8879\n",
    "# EPOCH : 21 | train_loss : -3.8916\n",
    "# EPOCH : 22 | train_loss : -3.8959\n",
    "# EPOCH : 23 | train_loss : -3.8989\n",
    "# EPOCH : 24 | train_loss : -3.9027\n",
    "# EPOCH : 25 | train_loss : -3.9056\n",
    "# EPOCH : 26 | train_loss : -3.9086\n",
    "# EPOCH : 27 | train_loss : -3.9118\n",
    "# EPOCH : 28 | train_loss : -3.9144\n",
    "# EPOCH : 29 | train_loss : -3.9176\n",
    "# EPOCH : 30 | train_loss : -3.9199\n",
    "# EPOCH : 31 | train_loss : -3.9221\n",
    "# EPOCH : 32 | train_loss : -3.9248\n",
    "# EPOCH : 33 | train_loss : -3.9270\n",
    "# EPOCH : 34 | train_loss : -3.9289\n",
    "# EPOCH : 35 | train_loss : -3.9310\n",
    "# EPOCH : 36 | train_loss : -3.9328\n",
    "# EPOCH : 37 | train_loss : -3.9343"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add H\n",
    "# 100%|██████████| 10113/10113 [2:39:32<00:00,  1.06it/s] \n",
    "# EPOCH : 1 | train_loss : -2.2254\n",
    "# 100%|██████████| 10113/10113 [2:40:06<00:00,  1.05it/s] \n",
    "# EPOCH : 2 | train_loss : -3.2680\n",
    "# 100%|██████████| 10113/10113 [2:40:23<00:00,  1.05it/s] \n",
    "# EPOCH : 3 | train_loss : -3.4840\n",
    "# 100%|██████████| 10113/10113 [2:40:28<00:00,  1.05it/s] \n",
    "# EPOCH : 4 | train_loss : -3.5963\n",
    "# 100%|██████████| 10113/10113 [2:40:39<00:00,  1.05it/s] \n",
    "# EPOCH : 5 | train_loss : -3.6643\n",
    "# 100%|██████████| 10113/10113 [2:41:22<00:00,  1.04it/s] \n",
    "# EPOCH : 5 | train_loss : -3.6644\n",
    "# 100%|██████████| 10113/10113 [2:43:59<00:00,  1.03it/s] \n",
    "# EPOCH : 6 | train_loss : -3.7121\n",
    "# 100%|██████████| 10113/10113 [2:43:19<00:00,  1.03it/s] \n",
    "# EPOCH : 7 | train_loss : -3.7456\n",
    "# 100%|██████████| 10113/10113 [2:43:16<00:00,  1.03it/s] \n",
    "# EPOCH : 8 | train_loss : -3.7727\n",
    "# 100%|██████████| 10113/10113 [2:42:57<00:00,  1.03it/s] \n",
    "# EPOCH : 9 | train_loss : -3.7932\n",
    "# 100%|██████████| 10113/10113 [2:41:36<00:00,  1.04it/s] \n",
    "# EPOCH : 10 | train_loss : -3.8113\n",
    "# 100%|██████████| 10113/10113 [2:42:50<00:00,  1.04it/s] \n",
    "# EPOCH : 11 | train_loss : -3.8263\n",
    "# 100%|██████████| 10113/10113 [2:42:50<00:00,  1.04it/s] \n",
    "# EPOCH : 12 | train_loss : -3.8376\n",
    "# 100%|██████████| 10113/10113 [2:43:01<00:00,  1.03it/s] \n",
    "# EPOCH : 13 | train_loss : -3.8482\n",
    "# 100%|██████████| 10113/10113 [2:43:19<00:00,  1.03it/s] \n",
    "# EPOCH : 14 | train_loss : -3.8564\n",
    "# 100%|██████████| 10113/10113 [2:42:09<00:00,  1.04it/s] \n",
    "# EPOCH : 15 | train_loss : -3.8636\n",
    "# 100%|██████████| 10113/10113 [2:42:25<00:00,  1.04it/s] \n",
    "# EPOCH : 16 | train_loss : -3.8719\n",
    "# 100%|██████████| 10113/10113 [2:42:51<00:00,  1.03it/s] \n",
    "# EPOCH : 17 | train_loss : -3.8782\n",
    "# 100%|██████████| 10113/10113 [2:42:59<00:00,  1.03it/s] \n",
    "# EPOCH : 18 | train_loss : -3.8842\n",
    "# 100%|██████████| 10113/10113 [2:43:17<00:00,  1.03it/s] \n",
    "# EPOCH : 19 | train_loss : -3.8888\n",
    "# 100%|██████████| 10113/10113 [2:43:05<00:00,  1.03it/s] \n",
    "# EPOCH : 20 | train_loss : -3.8924"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
