{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, normalize\n",
    "import torch, os, random, copy\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from modules.ogb.graph_aug import mask_nodes, mask_edges, permute_edges, drop_nodes, subgraph\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot as plt\n",
    "from torch_ema import ExponentialMovingAverage\n",
    "from modules.ogb.utils import smiles2graph\n",
    "from modules.dualgraph.mol import smiles2graphwithface, simles2graphwithface_with_mask\n",
    "from modules.dualgraph.gnn import one_hot_atoms, one_hot_bonds, GNN2\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import Chem\n",
    "from torch_geometric.data import Dataset, InMemoryDataset\n",
    "from modules.dualgraph.dataset import DGData\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from rdkit.Chem import PandasTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import repeat\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import subgraph, to_networkx\n",
    "\n",
    "class MoleculeDataset_graphcl(InMemoryDataset):\n",
    "    def __init__(self,\n",
    "                 root='dataset_path',\n",
    "                 transform=None,\n",
    "                 pre_transform=None, \n",
    "                 df=None):\n",
    "        self.df = df\n",
    "        self.aug_prob = None\n",
    "        self.aug_mode = 'sample'\n",
    "        self.aug_strength = 0.2\n",
    "        self.augmentations = [self.node_drop, self.subgraph,\n",
    "                              self.edge_pert, self.attr_mask, lambda x: x]\n",
    "        super().__init__(root, transform, pre_transform, df)\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [f'raw_{i+1}.pt' for i in range(self.df.shape[0])]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [f'data_{i+1}.pt' for i in range(self.df.shape[0])]\n",
    "    \n",
    "    def set_augMode(self, aug_mode):\n",
    "        self.aug_mode = aug_mode\n",
    "\n",
    "    def set_augStrength(self, aug_strength):\n",
    "        self.aug_strength = aug_strength\n",
    "\n",
    "    def set_augProb(self, aug_prob):\n",
    "        self.aug_prob = aug_prob\n",
    "\n",
    "    def node_drop(self, data):\n",
    "        node_num, _ = data.x.size()\n",
    "        _, edge_num = data.edge_index.size()\n",
    "        drop_num = int(node_num * self.aug_strength)\n",
    "\n",
    "        idx_perm = np.random.permutation(node_num)\n",
    "        idx_nodrop = idx_perm[drop_num:].tolist()\n",
    "        idx_nodrop.sort()\n",
    "\n",
    "        node_mask = torch.zeros(node_num).bool()\n",
    "        node_mask[~torch.tensor(idx_nodrop)]=True\n",
    "\n",
    "        edge_idx, edge_attr, edge_mask = subgraph(subset=idx_nodrop,\n",
    "                                        edge_index=data.edge_index,\n",
    "                                        edge_attr=data.edge_attr,\n",
    "                                        relabel_nodes=True,\n",
    "                                        num_nodes=node_num,\n",
    "                                        return_edge_mask=True)\n",
    "        data = self.ring_drop(data, node_mask, edge_mask)\n",
    "        \n",
    "        data.edge_index = edge_idx\n",
    "        data.edge_attr = edge_attr\n",
    "        data.x = data.x[idx_nodrop]        \n",
    "\n",
    "        data.__num_nodes__, _ = data.x.shape\n",
    "        return data\n",
    "\n",
    "    def edge_pert(self, data):        \n",
    "        node_num, _ = data.x.size()\n",
    "        _, edge_num = data.edge_index.size()\n",
    "        pert_num = int(edge_num * self.aug_strength)\n",
    "\n",
    "        # delete edges\n",
    "        idx_drop = np.random.choice(edge_num, (edge_num - pert_num),\n",
    "                                    replace=False)\n",
    "        edge_index = data.edge_index[:, idx_drop]\n",
    "        edge_attr = data.edge_attr[idx_drop]\n",
    "\n",
    "        # add edges\n",
    "        adj = torch.ones((node_num, node_num))\n",
    "        adj[edge_index[0], edge_index[1]] = 0\n",
    "        # edge_index_nonexist = adj.nonzero(as_tuple=False).t()\n",
    "        edge_index_nonexist = torch.nonzero(adj, as_tuple=False).t()\n",
    "        idx_add = np.random.choice(edge_index_nonexist.shape[1],\n",
    "                                    pert_num, replace=False)\n",
    "        edge_index_add = edge_index_nonexist[:, idx_add]\n",
    "        # random 4-class & 3-class edge_attr for 1st & 2nd dimension\n",
    "        edge_attr_add_1 = torch.tensor(np.random.randint(\n",
    "            4, size=(edge_index_add.shape[1], 1)))\n",
    "        edge_attr_add_2 = torch.tensor(np.random.randint(\n",
    "            3, size=(edge_index_add.shape[1], 1)))\n",
    "        edge_attr_add_3 = torch.tensor(np.random.randint(\n",
    "            2, size=(edge_index_add.shape[1], 1)))\n",
    "        edge_attr_add = torch.cat((edge_attr_add_1, edge_attr_add_2, edge_attr_add_3), dim=1)\n",
    "        edge_index = torch.cat((edge_index, edge_index_add), dim=1)\n",
    "        \n",
    "\n",
    "        edge_attr = torch.cat((edge_attr, edge_attr_add), dim=0)\n",
    "\n",
    "        data.edge_index = edge_index\n",
    "        data.edge_attr = edge_attr\n",
    "        return data\n",
    "\n",
    "    def attr_mask(self, data):\n",
    "\n",
    "        _x = data.x.clone()\n",
    "        node_num, _ = data.x.size()\n",
    "        mask_num = int(node_num * self.aug_strength)\n",
    "\n",
    "        token = data.x.float().mean(dim=0).long()\n",
    "        idx_mask = np.random.choice(\n",
    "            node_num, mask_num, replace=False)\n",
    "\n",
    "        _x[idx_mask] = token\n",
    "        data.x = _x\n",
    "        return data    \n",
    "    \n",
    "    def ring_drop(self, graph, node_mask, edge_mask):\n",
    "        node_mask_idx = (node_mask.cumsum(0) - 1) * node_mask\n",
    "        edge_mask_idx = (edge_mask.cumsum(0) - 1) * edge_mask\n",
    "\n",
    "        graph.ring_index = graph.ring_index[:, edge_mask]\n",
    "\n",
    "        graph.n_edges = edge_mask.sum().item()\n",
    "        graph.n_nodes = node_mask.sum().item()\n",
    "\n",
    "        nf_node_mask = ~torch.isin(graph.nf_node, torch.where(~node_mask)[0])[0]\n",
    "        # for ridx in range(graph.num_rings):        \n",
    "        #     ring_node_mask = nf_node_mask[graph.nf_ring[0] == ridx+1]\n",
    "        #     if not ring_node_mask.all():\n",
    "        #         graph.ring_mask[ridx] = False\n",
    "\n",
    "        graph.nf_node = node_mask_idx[graph.nf_node[:, nf_node_mask]]\n",
    "        graph.nf_ring = graph.nf_ring[:, nf_node_mask]\n",
    "        graph.n_nfs = graph.nf_node.size(1)\n",
    "        return graph\n",
    "    \n",
    "    def subgraph(self, data):\n",
    "\n",
    "        G = to_networkx(data)\n",
    "        node_num, _ = data.x.size()\n",
    "        _, edge_num = data.edge_index.size()\n",
    "        sub_num = int(node_num * (1 - self.aug_strength))\n",
    "\n",
    "        idx_sub = [np.random.randint(node_num, size=1)[0]]\n",
    "        idx_neigh = set([n for n in G.neighbors(idx_sub[-1])])\n",
    "\n",
    "        while len(idx_sub) <= sub_num:\n",
    "            if len(idx_neigh) == 0:\n",
    "                idx_unsub = list(set([n for n in range(node_num)]).difference(set(idx_sub)))\n",
    "                idx_neigh = set([np.random.choice(idx_unsub)])\n",
    "            sample_node = np.random.choice(list(idx_neigh))\n",
    "\n",
    "            idx_sub.append(sample_node)\n",
    "            idx_neigh = idx_neigh.union(\n",
    "                set([n for n in G.neighbors(idx_sub[-1])])).difference(set(idx_sub))\n",
    "\n",
    "        idx_nondrop = idx_sub\n",
    "        idx_nondrop.sort()\n",
    "        \n",
    "        node_mask = torch.zeros(node_num).bool()\n",
    "        node_mask[~torch.tensor(idx_nondrop)]=True\n",
    "\n",
    "        edge_idx, edge_attr, edge_mask = subgraph(subset=idx_nondrop,\n",
    "                                       edge_index=data.edge_index,\n",
    "                                       edge_attr=data.edge_attr,\n",
    "                                       relabel_nodes=True,\n",
    "                                       num_nodes=node_num,\n",
    "                                       return_edge_mask=True)\n",
    "        data = self.ring_drop(data, node_mask, edge_mask)\n",
    "\n",
    "        data.edge_index = edge_idx\n",
    "        data.edge_attr = edge_attr\n",
    "        data.x = data.x[idx_nondrop]\n",
    "        data.__num_nodes__, _ = data.x.shape\n",
    "        return data\n",
    "\n",
    "    def get(self, idx):\n",
    "        sid = self.sid_list[idx]        \n",
    "        data = torch.load(f'graph_pt/{sid}_addh.pt')\n",
    "        data1 = copy.deepcopy(data)\n",
    "        data2 = copy.deepcopy(data)\n",
    "\n",
    "        if self.aug_mode == 'no_aug':\n",
    "            n_aug1, n_aug2 = 4, 4\n",
    "            data1 = self.augmentations[n_aug1](data1)\n",
    "            data2 = self.augmentations[n_aug2](data2)\n",
    "        elif self.aug_mode == 'uniform':\n",
    "            n_aug = np.random.choice(25, 1)[0]\n",
    "            n_aug1, n_aug2 = n_aug // 5, n_aug % 5\n",
    "            data1 = self.augmentations[n_aug1](data1)\n",
    "            data2 = self.augmentations[n_aug2](data2)\n",
    "        elif self.aug_mode == 'sample':\n",
    "            n_aug = np.random.choice(25, 1, p=self.aug_prob)[0]\n",
    "            n_aug1, n_aug2 = n_aug // 5, n_aug % 5\n",
    "            data1 = self.augmentations[n_aug1](data1)\n",
    "            data2 = self.augmentations[n_aug2](data2)\n",
    "        else:\n",
    "            raise ValueError\n",
    "        \n",
    "        return data, data1, data2\n",
    "    \n",
    "    def process(self):\n",
    "        self.sid_list = []        \n",
    "        \n",
    "        for i in range(self.df.shape[0]):\n",
    "            sid = self.df.loc[i, 'ID']            \n",
    "            self.sid_list.append(sid)\n",
    "    def len(self):\n",
    "        return self.df.shape[0]            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PandasTools.LoadSDF('/home/pjh/CODE/mol/Site-of-metabolism/Site-of-metabolism/data/train_0611.sdf')\n",
    "data['ID'] = 'TRAIN' + data.index.astype(str).str.zfill(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = torch.load('graph_pt/TRAIN0000_addh.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = False  # type: ignore\n",
    "seed_everything(2023)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MoleculeDataset_graphcl(df = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_augMode('sample')\n",
    "train_dataset.set_augProb(np.ones(25) / 25)\n",
    "train_dataset.set_augStrength(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch1.ring_mask\n",
    "# batch1.nf_node\n",
    "# batch1.nf_ring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_loss = nn.TripletMarginLoss(margin=0.0, p=2)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphContrastiveLearning(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GraphContrastiveLearning, self).__init__()\n",
    "        self.ddi = True\n",
    "        self.gnn = GNN2(\n",
    "                        mlp_hidden_size = 512,\n",
    "                        mlp_layers = 2,\n",
    "                        num_message_passing_steps=8,\n",
    "                        latent_size = 128,\n",
    "                        use_layer_norm = True,\n",
    "                        use_bn=False,\n",
    "                        use_face=True,\n",
    "                        som_mode=False,\n",
    "                        ddi=True,\n",
    "                        dropedge_rate = 0.1,\n",
    "                        dropnode_rate = 0.1,\n",
    "                        dropout = 0.1,\n",
    "                        dropnet = 0.1,\n",
    "                        global_reducer = 'sum',\n",
    "                        node_reducer = 'sum',\n",
    "                        face_reducer = 'sum',\n",
    "                        graph_pooling = 'sum',                        \n",
    "                        node_attn = True,\n",
    "                        face_attn = True,\n",
    "                        encoder_dropout=0.0,                        \n",
    "                        )\n",
    "                        \n",
    "        self.proj = nn.Sequential(\n",
    "                        nn.Linear(128, 128),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(128, 128),\n",
    "                        )\n",
    "    def forward(self, batch):\n",
    "        mol = self.gnn(batch).squeeze(1)\n",
    "        return self.proj(mol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "epochs = 100\n",
    "lr = 1e-5\n",
    "weight_decay = 5e-5\n",
    "batch_size= 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MoleculeDataset_graphcl(df = data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers = 0)\n",
    "\n",
    "model = GraphContrastiveLearning().to(device)\n",
    "\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "ema = ExponentialMovingAverage(model.parameters(), decay=0.999)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=epochs, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 67/246 [00:08<00:22,  8.13it/s]"
     ]
    }
   ],
   "source": [
    "for batch, batch1, batch2 in tqdm(train_loader):\n",
    "    with torch.no_grad():\n",
    "        out =  model(batch.to(device))\n",
    "        out1 = model(batch1.to(device))\n",
    "        out2 = model(batch2.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_cl(x1, x2):\n",
    "    T = 0.1\n",
    "    batch_size, _ = x1.size()\n",
    "    x1_abs = x1.norm(dim=1)\n",
    "    x2_abs = x2.norm(dim=1)\n",
    "\n",
    "    sim_matrix = torch.einsum('ik,jk->ij', x1, x2) / torch.einsum('i,j->ij', x1_abs, x2_abs)\n",
    "    sim_matrix = torch.exp(sim_matrix / T)\n",
    "    pos_sim = sim_matrix[range(batch_size), range(batch_size)]\n",
    "    loss = pos_sim / (sim_matrix.sum(dim=1) - pos_sim)\n",
    "    loss = - torch.log(loss).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(f'/home/pjh/workspace/SOM/graph_mamba/ckpt_pretrain/gnn_pretrain_epoch36.pt')\n",
    "\n",
    "model.load_state_dict(state_dict['model_state_dict'])\n",
    "scheduler.load_state_dict(state_dict['scheduler_state_dict'])\n",
    "ema.load_state_dict(state_dict['ema_state_dict'] )\n",
    "optim.load_state_dict(state_dict['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = 1e6\n",
    "start = 1\n",
    "for epoch in range(37, epochs+1):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        batch = [bat.to(device) for bat in batch]\n",
    "        outputs = [model(bat) for  bat in batch]        \n",
    "        origin_output = outputs[0]\n",
    "        \n",
    "        mask_cl_loss = loss_cl(outputs[1], outputs[2])                    \n",
    "        mask_t_loss = triplet_loss(outputs[0], outputs[1], outputs[2])\n",
    "\n",
    "        loss = mask_cl_loss + (mask_t_loss * 0.1)\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward() \n",
    "        optim.step()\n",
    "        ema.update()\n",
    "        \n",
    "        train_loss += loss.cpu().item()\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if train_loss < best_val_loss:\n",
    "        best_val_loss = train_loss\n",
    "        torch.save(model.gnn.state_dict(), f'ckpt_pretrain/gnn_pretrain.pt')\n",
    "        \n",
    "\n",
    "    scheduler.step()\n",
    "    torch.save(\n",
    "            {\n",
    "            'optimizer_state_dict': optim.state_dict(),\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'gnn_state_dict' : model.gnn.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'ema_state_dict' : ema.state_dict()\n",
    "            },\n",
    "            f'ckpt_pretrain/gnn_pretrain_epoch{epoch}.pt')\n",
    "\n",
    "    print(f'EPOCH : {epoch} | train_loss : {train_loss/len(train_loader):.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCH : 1 | train_loss : -2.3468\n",
    "# EPOCH : 2 | train_loss : -3.3075\n",
    "# EPOCH : 3 | train_loss : -3.5078\n",
    "# EPOCH : 4 | train_loss : -3.6116\n",
    "# EPOCH : 5 | train_loss : -3.6767\n",
    "# EPOCH : 6 | train_loss : -3.7220\n",
    "# EPOCH : 7 | train_loss : -3.7547\n",
    "# EPOCH : 8 | train_loss : -3.7785\n",
    "# EPOCH : 9 | train_loss : -3.7976\n",
    "# EPOCH : 10 | train_loss : -3.8130\n",
    "# EPOCH : 11 | train_loss : -3.8248\n",
    "# EPOCH : 12 | train_loss : -3.8358\n",
    "# EPOCH : 13 | train_loss : -3.8443\n",
    "# EPOCH : 14 | train_loss : -3.8521\n",
    "# EPOCH : 15 | train_loss : -3.8593\n",
    "# EPOCH : 16 | train_loss : -3.8657\n",
    "# EPOCH : 17 | train_loss : -3.8720\n",
    "# EPOCH : 18 | train_loss : -3.8774\n",
    "# EPOCH : 19 | train_loss : -3.8831\n",
    "# EPOCH : 20 | train_loss : -3.8879\n",
    "# EPOCH : 21 | train_loss : -3.8916\n",
    "# EPOCH : 22 | train_loss : -3.8959\n",
    "# EPOCH : 23 | train_loss : -3.8989\n",
    "# EPOCH : 24 | train_loss : -3.9027\n",
    "# EPOCH : 25 | train_loss : -3.9056\n",
    "# EPOCH : 26 | train_loss : -3.9086\n",
    "# EPOCH : 27 | train_loss : -3.9118\n",
    "# EPOCH : 28 | train_loss : -3.9144\n",
    "# EPOCH : 29 | train_loss : -3.9176\n",
    "# EPOCH : 30 | train_loss : -3.9199\n",
    "# EPOCH : 31 | train_loss : -3.9221\n",
    "# EPOCH : 32 | train_loss : -3.9248\n",
    "# EPOCH : 33 | train_loss : -3.9270\n",
    "# EPOCH : 34 | train_loss : -3.9289\n",
    "# EPOCH : 35 | train_loss : -3.9310\n",
    "# EPOCH : 36 | train_loss : -3.9328\n",
    "# EPOCH : 37 | train_loss : -3.9343"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add H\n",
    "# 100%|██████████| 10113/10113 [2:39:32<00:00,  1.06it/s] \n",
    "# EPOCH : 1 | train_loss : -2.2254\n",
    "# 100%|██████████| 10113/10113 [2:40:06<00:00,  1.05it/s] \n",
    "# EPOCH : 2 | train_loss : -3.2680\n",
    "# 100%|██████████| 10113/10113 [2:40:23<00:00,  1.05it/s] \n",
    "# EPOCH : 3 | train_loss : -3.4840\n",
    "# 100%|██████████| 10113/10113 [2:40:28<00:00,  1.05it/s] \n",
    "# EPOCH : 4 | train_loss : -3.5963\n",
    "# 100%|██████████| 10113/10113 [2:40:39<00:00,  1.05it/s] \n",
    "# EPOCH : 5 | train_loss : -3.6643\n",
    "# 100%|██████████| 10113/10113 [2:41:22<00:00,  1.04it/s] \n",
    "# EPOCH : 5 | train_loss : -3.6644\n",
    "# 100%|██████████| 10113/10113 [2:43:59<00:00,  1.03it/s] \n",
    "# EPOCH : 6 | train_loss : -3.7121\n",
    "# 100%|██████████| 10113/10113 [2:43:19<00:00,  1.03it/s] \n",
    "# EPOCH : 7 | train_loss : -3.7456\n",
    "# 100%|██████████| 10113/10113 [2:43:16<00:00,  1.03it/s] \n",
    "# EPOCH : 8 | train_loss : -3.7727\n",
    "# 100%|██████████| 10113/10113 [2:42:57<00:00,  1.03it/s] \n",
    "# EPOCH : 9 | train_loss : -3.7932\n",
    "# 100%|██████████| 10113/10113 [2:41:36<00:00,  1.04it/s] \n",
    "# EPOCH : 10 | train_loss : -3.8113\n",
    "# 100%|██████████| 10113/10113 [2:42:50<00:00,  1.04it/s] \n",
    "# EPOCH : 11 | train_loss : -3.8263\n",
    "# 100%|██████████| 10113/10113 [2:42:50<00:00,  1.04it/s] \n",
    "# EPOCH : 12 | train_loss : -3.8376\n",
    "# 100%|██████████| 10113/10113 [2:43:01<00:00,  1.03it/s] \n",
    "# EPOCH : 13 | train_loss : -3.8482\n",
    "# 100%|██████████| 10113/10113 [2:43:19<00:00,  1.03it/s] \n",
    "# EPOCH : 14 | train_loss : -3.8564\n",
    "# 100%|██████████| 10113/10113 [2:42:09<00:00,  1.04it/s] \n",
    "# EPOCH : 15 | train_loss : -3.8636\n",
    "# 100%|██████████| 10113/10113 [2:42:25<00:00,  1.04it/s] \n",
    "# EPOCH : 16 | train_loss : -3.8719\n",
    "# 100%|██████████| 10113/10113 [2:42:51<00:00,  1.03it/s] \n",
    "# EPOCH : 17 | train_loss : -3.8782\n",
    "# 100%|██████████| 10113/10113 [2:42:59<00:00,  1.03it/s] \n",
    "# EPOCH : 18 | train_loss : -3.8842\n",
    "# 100%|██████████| 10113/10113 [2:43:17<00:00,  1.03it/s] \n",
    "# EPOCH : 19 | train_loss : -3.8888\n",
    "# 100%|██████████| 10113/10113 [2:43:05<00:00,  1.03it/s] \n",
    "# EPOCH : 20 | train_loss : -3.8924"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# +--------------+------------+------------+------------+------------+-----------+-----------+-----------+----------+-----------+-----------+-----------+------------+\n",
    "# | CYP          |   jac_bond |   f1s_bond |   apc_bond | n_bond     |   jac_spn |   f1s_spn |   apc_spn | n_spn    |   jac_som |   f1s_som |   apc_som | n_som      |\n",
    "# +==============+============+============+============+============+===========+===========+===========+==========+===========+===========+===========+============+\n",
    "# | BOM_1A2      |     0.1907 |     0.3203 |     0.2288 | 106 / 5772 |    0.1429 |    0.2500 |    0.2953 | 5 / 399  |    0.1892 |    0.3182 |    0.2252 | 111 / 6171 |\n",
    "# +--------------+------------+------------+------------+------------+-----------+-----------+-----------+----------+-----------+-----------+-----------+------------+\n",
    "# | BOM_2A6      |     0.1791 |     0.3038 |     0.2654 | 40 / 5772  |    0.0000 |    0.0000 |    0.0776 | 3 / 399  |    0.1714 |    0.2927 |    0.2501 | 43 / 6171  |\n",
    "# +--------------+------------+------------+------------+------------+-----------+-----------+-----------+----------+-----------+-----------+-----------+------------+\n",
    "# | BOM_2B6      |     0.0828 |     0.1529 |     0.0952 | 49 / 5772  |    0.0000 |    0.0000 |    0.2849 | 5 / 399  |    0.0802 |    0.1486 |    0.0925 | 54 / 6171  |\n",
    "# +--------------+------------+------------+------------+------------+-----------+-----------+-----------+----------+-----------+-----------+-----------+------------+\n",
    "# | BOM_2C8      |     0.1257 |     0.2233 |     0.1366 | 91 / 5772  |    0.0000 |    0.0000 |    0.0174 | 3 / 399  |    0.1237 |    0.2202 |    0.1327 | 94 / 6171  |\n",
    "# +--------------+------------+------------+------------+------------+-----------+-----------+-----------+----------+-----------+-----------+-----------+------------+\n",
    "# | BOM_2C9      |     0.1125 |     0.2022 |     0.1489 | 82 / 5772  |    0.2500 |    0.4000 |    0.3667 | 7 / 399  |    0.1169 |    0.2094 |    0.1457 | 89 / 6171  |\n",
    "# +--------------+------------+------------+------------+------------+-----------+-----------+-----------+----------+-----------+-----------+-----------+------------+\n",
    "# | BOM_2C19     |     0.1354 |     0.2385 |     0.1494 | 79 / 5772  |    0.0000 |    0.0000 |    0.4628 | 7 / 399  |    0.1314 |    0.2322 |    0.1469 | 86 / 6171  |\n",
    "# +--------------+------------+------------+------------+------------+-----------+-----------+-----------+----------+-----------+-----------+-----------+------------+\n",
    "# | BOM_2D6      |     0.2328 |     0.3776 |     0.2657 | 133 / 5772 |    0.0000 |    0.0000 |    0.3995 | 8 / 399  |    0.2250 |    0.3673 |    0.2633 | 141 / 6171 |\n",
    "# +--------------+------------+------------+------------+------------+-----------+-----------+-----------+----------+-----------+-----------+-----------+------------+\n",
    "# | BOM_2E1      |     0.2537 |     0.4048 |     0.3449 | 47 / 5772  |    0.0000 |    0.0000 |    0.1081 | 4 / 399  |    0.2394 |    0.3864 |    0.3265 | 51 / 6171  |\n",
    "# +--------------+------------+------------+------------+------------+-----------+-----------+-----------+----------+-----------+-----------+-----------+------------+\n",
    "# | BOM_3A4      |     0.2350 |     0.3805 |     0.3263 | 276 / 5772 |    0.0667 |    0.1250 |    0.2614 | 14 / 399 |    0.2303 |    0.3744 |    0.3191 | 290 / 6171 |\n",
    "# +--------------+------------+------------+------------+------------+-----------+-----------+-----------+----------+-----------+-----------+-----------+------------+\n",
    "# | CYP_REACTION |     0.3712 |     0.5415 |     0.5213 | 417 / 5772 |    0.2400 |    0.3871 |    0.2820 | 19 / 399 |    0.3663 |    0.5362 |    0.5111 | 436 / 6171 |\n",
    "# +--------------+------------+------------+------------+------------+-----------+-----------+-----------+----------+-----------+-----------+-----------+------------+\n",
    "# +--------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-----------+\n",
    "# | CYP          |   jac_hdx |   f1s_hdx |   apc_hdx | n_hdx      |   jac_oxi |   f1s_oxi |   apc_oxi | n_oxi      |   jac_clv |   f1s_clv |   apc_clv | n_clv      |   jac_rdc |   f1s_rdc |   apc_rdc | n_rdc     |\n",
    "# +==============+===========+===========+===========+============+===========+===========+===========+============+===========+===========+===========+============+===========+===========+===========+===========+\n",
    "# | BOM_1A2      |    0.2097 |    0.3467 |    0.2576 | 56 / 1995  |    0.1750 |    0.2979 |    0.2319 | 43 / 5772  |    0.2024 |    0.3366 |    0.2826 | 37 / 3777  |    0.0000 |    0.0000 |    0.1736 | 2 / 3777  |\n",
    "# +--------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-----------+\n",
    "# | BOM_2A6      |    0.2286 |    0.3721 |    0.3068 | 17 / 1995  |    0.1724 |    0.2941 |    0.2553 | 19 / 5772  |    0.0000 |    0.0000 |    0.2840 | 21 / 3777  |    0.0000 |    0.0000 |   -0.0000 | 0 / 3777  |\n",
    "# +--------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-----------+\n",
    "# | BOM_2B6      |    0.1154 |    0.2069 |    0.1252 | 24 / 1995  |    0.0333 |    0.0645 |    0.0859 | 22 / 5772  |    0.0328 |    0.0635 |    0.1410 | 17 / 3777  |    0.0000 |    0.0000 |   -0.0000 | 0 / 3777  |\n",
    "# +--------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-----------+\n",
    "# | BOM_2C8      |    0.1240 |    0.2206 |    0.1306 | 59 / 1995  |    0.1286 |    0.2278 |    0.1572 | 31 / 5772  |    0.1194 |    0.2133 |    0.2064 | 26 / 3777  |    0.0000 |    0.0000 |   -0.0000 | 0 / 3777  |\n",
    "# +--------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-----------+\n",
    "# | BOM_2C9      |    0.0919 |    0.1683 |    0.1598 | 46 / 1995  |    0.0886 |    0.1628 |    0.1611 | 31 / 5772  |    0.1571 |    0.2716 |    0.2537 | 23 / 3777  |    0.0000 |    0.0000 |    0.0497 | 3 / 3777  |\n",
    "# +--------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-----------+\n",
    "# | BOM_2C19     |    0.1083 |    0.1954 |    0.1559 | 43 / 1995  |    0.1538 |    0.2667 |    0.1508 | 31 / 5772  |    0.1733 |    0.2955 |    0.2411 | 27 / 3777  |    0.0000 |    0.0000 |    0.2429 | 2 / 3777  |\n",
    "# +--------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-----------+\n",
    "# | BOM_2D6      |    0.1731 |    0.2951 |    0.2224 | 77 / 1995  |    0.2778 |    0.4348 |    0.2831 | 48 / 5772  |    0.3500 |    0.5185 |    0.3478 | 45 / 3777  |    0.0000 |    0.0000 |    0.1949 | 3 / 3777  |\n",
    "# +--------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-----------+\n",
    "# | BOM_2E1      |    0.2059 |    0.3415 |    0.3988 | 21 / 1995  |    0.2500 |    0.4000 |    0.2891 | 24 / 5772  |    0.0000 |    0.0000 |    0.1906 | 13 / 3777  |    0.0000 |    0.0000 |    0.2429 | 2 / 3777  |\n",
    "# +--------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-----------+\n",
    "# | BOM_3A4      |    0.2104 |    0.3476 |    0.3099 | 146 / 1995 |    0.2690 |    0.4240 |    0.3440 | 112 / 5772 |    0.3121 |    0.4757 |    0.3667 | 97 / 3777  |    0.0000 |    0.0000 |    0.1316 | 9 / 3777  |\n",
    "# +--------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-----------+\n",
    "# | CYP_REACTION |    0.3341 |    0.5009 |    0.4879 | 229 / 1995 |    0.4167 |    0.5882 |    0.5132 | 162 / 5772 |    0.4355 |    0.6067 |    0.5908 | 147 / 3777 |    0.0000 |    0.0000 |    0.1445 | 11 / 3777 |\n",
    "# +--------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+------------+-----------+-----------+-----------+-----------+"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
